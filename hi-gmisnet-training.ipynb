{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8610636,"sourceType":"datasetVersion","datasetId":5152851}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hyperparameters (Learning rate, Scheduler, etc.) need to be optimized per dataset to get the best results. Use tensorflow version 2.8.4 and python version 3.7","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_wavelets -q\n!pip install natsort -q\nfrom keras.utils.generic_utils import get_custom_objects\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randint\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\nfrom keras import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.utils import plot_model\nimport numpy\nfrom PIL import Image, ImageOps\nimport os\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Activation, add\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input, GlobalAveragePooling2D, Multiply\n\nfrom keras.models import Model, model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import ELU, LeakyReLU\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras import backend as K\nimport random\nimport albumentations as A\nimport natsort\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport glob\nimport pandas as pd\nimport pywt\nfrom tensorflow_wavelets.utils.helpers import *\nfrom tensorflow.keras.applications import VGG16,DenseNet201","metadata":{"papermill":{"duration":17.673742,"end_time":"2023-01-11T09:46:28.721252","exception":false,"start_time":"2023-01-11T09:46:11.04751","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-03-05T16:46:28.524722Z","iopub.execute_input":"2025-03-05T16:46:28.525041Z","iopub.status.idle":"2025-03-05T16:47:13.144195Z","shell.execute_reply.started":"2025-03-05T16:46:28.524971Z","shell.execute_reply":"2025-03-05T16:47:13.143048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\nBATCH_SIZE = 2\nEPOCHS = 200\nTRAIN_IMAGE_PATH = '/kaggle/input/medical-image-segmentation-datasets-hi-gmisnet/Hi-gMISnet_all_dataset/BUSI/train_folder/img'\nTRAIN_MASK_PATH = '/kaggle/input/medical-image-segmentation-datasets-hi-gmisnet/Hi-gMISnet_all_dataset/BUSI/train_folder/label'\nVAL_IMAGE_PATH = '/kaggle/input/medical-image-segmentation-datasets-hi-gmisnet/Hi-gMISnet_all_dataset/BUSI/val_folder/img'\nVAL_MASK_PATH = '/kaggle/input/medical-image-segmentation-datasets-hi-gmisnet/Hi-gMISnet_all_dataset/BUSI/val_folder/label'\nTEST_IMAGE_PATH = '/kaggle/input/medical-image-segmentation-datasets-hi-gmisnet/Hi-gMISnet_all_dataset/BUSI/test_folder/img'\nTEST_MASK_PATH = '/kaggle/input/medical-image-segmentation-datasets-hi-gmisnet/Hi-gMISnet_all_dataset/BUSI/test_folder/label'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:47:13.14659Z","iopub.execute_input":"2025-03-05T16:47:13.147217Z","iopub.status.idle":"2025-03-05T16:47:13.153177Z","shell.execute_reply.started":"2025-03-05T16:47:13.147186Z","shell.execute_reply":"2025-03-05T16:47:13.152133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DWT pooling and Convolution Block","metadata":{}},{"cell_type":"code","source":"class DWT(layers.Layer):\n\n    def __init__(self, wavelet_name='haar', concat=1, **kwargs):\n        super(DWT, self).__init__(**kwargs)\n        # get filter coeffs from 3rd party lib\n        wavelet = pywt.Wavelet(wavelet_name)\n        self.dec_len = wavelet.dec_len\n        self.concat = concat\n        # decomposition filter low pass and hight pass coeffs\n        db2_lpf = wavelet.dec_lo\n        db2_hpf = wavelet.dec_hi\n\n        # covert filters into tensors and reshape for convolution math\n        db2_lpf = tf.constant(db2_lpf[::-1])\n        self.db2_lpf = tf.reshape(db2_lpf, (1, wavelet.dec_len, 1, 1))\n\n        db2_hpf = tf.constant(db2_hpf[::-1])\n        self.db2_hpf = tf.reshape(db2_hpf, (1, wavelet.dec_len, 1, 1))\n\n        self.conv_type = \"VALID\"\n        self.border_padd = \"SYMMETRIC\"\n        self.wavelet_name = wavelet_name\n        self.concat = concat\n\n    def build(self, input_shape):\n        # filter dims should be bigger if input is not gray scale\n        if input_shape[-1] != 1:\n            # self.db2_lpf = tf.repeat(self.db2_lpf, input_shape[-1], axis=-1)\n            self.db2_lpf = tf.keras.backend.repeat_elements(self.db2_lpf, input_shape[-1], axis=-1)\n            # self.db2_hpf = tf.repeat(self.db2_hpf, input_shape[-1], axis=-1)\n            self.db2_hpf = tf.keras.backend.repeat_elements(self.db2_hpf, input_shape[-1], axis=-1)\n\n    def call(self, inputs, training=None, mask=None):\n\n        # border padding symatric add coulums\n        inputs_pad = tf.pad(inputs, [[0, 0], [0, 0], [self.dec_len-1, self.dec_len-1], [0, 0]], self.border_padd)\n\n        # approximation conv only rows\n        a = tf.nn.conv2d(\n            inputs_pad, self.db2_lpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # details conv only rows\n        d = tf.nn.conv2d(\n            inputs_pad, self.db2_hpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # ds - down sample\n        a_ds = a[:, :, 1:a.shape[2]:2, :]\n        d_ds = d[:, :, 1:d.shape[2]:2, :]\n\n        # border padding symatric add rows\n        a_ds_pad = tf.pad(a_ds, [[0, 0], [self.dec_len-1, self.dec_len-1], [0, 0], [0, 0]], self.border_padd)\n        d_ds_pad = tf.pad(d_ds, [[0, 0], [self.dec_len-1, self.dec_len-1], [0, 0], [0, 0]], self.border_padd)\n\n        # convolution is done on the rows so we need to\n        # transpose the matrix in order to convolve the colums\n        a_ds_pad = tf.transpose(a_ds_pad, perm=[0, 2, 1, 3])\n        d_ds_pad = tf.transpose(d_ds_pad, perm=[0, 2, 1, 3])\n\n        # aa approximation approximation\n        aa = tf.nn.conv2d(\n            a_ds_pad, self.db2_lpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # ad approximation details\n        ad = tf.nn.conv2d(\n            a_ds_pad, self.db2_hpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # ad details aproximation\n        da = tf.nn.conv2d(\n            d_ds_pad, self.db2_lpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # dd details details\n        dd = tf.nn.conv2d(\n            d_ds_pad, self.db2_hpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n\n        # transpose back the matrix\n        aa = tf.transpose(aa, perm=[0, 2, 1, 3])\n        ad = tf.transpose(ad, perm=[0, 2, 1, 3])\n        da = tf.transpose(da, perm=[0, 2, 1, 3])\n        dd = tf.transpose(dd, perm=[0, 2, 1, 3])\n\n        # down sample\n        ll = aa[:, 1:aa.shape[1]:2, :, :]\n        lh = ad[:, 1:ad.shape[1]:2, :, :]\n        hl = da[:, 1:da.shape[1]:2, :, :]\n        hh = dd[:, 1:dd.shape[1]:2, :, :]\n\n        # concate all outputs ionto tensor\n        if self.concat == 0:\n            x = tf.concat([ll, lh, hl, hh], axis=-1)\n        elif self.concat == 2:\n            x = ll\n        elif self.concat ==1:\n            return ll,lh,hl,hh\n        else:\n            x = tf.concat([tf.concat([ll, lh], axis=1), tf.concat([hl, hh], axis=1)], axis=2)\n        return x\n    def get_config(self):\n        config = super(DWT, self).get_config()\n        config.update({'wavelet_name': self.wavelet_name, 'concat': self.concat})\n        return config\n\n\n\ndef wavelet_conv_block(x, in_channels,name_prefix=''):\n    \n    ll, lh, hl, hh = DWT(concat=1)(x)\n    \n    y = tf.concat([ll, lh, hl, hh], axis=3)\n    \n    conv1 = tf.keras.layers.Conv2D(in_channels * 2, kernel_size=1, dilation_rate=1, padding='valid')(y)\n    \n    conv2 = tf.keras.layers.Conv2D(in_channels, kernel_size=3, dilation_rate=1, padding='same')(conv1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2,training=True)\n    conv2 = tf.keras.layers.ReLU()(conv2)\n\n    \n    conv3 = tf.keras.layers.Conv2D(in_channels, kernel_size=5, dilation_rate=1, padding='same')(conv1)\n    conv3 = tf.keras.layers.BatchNormalization()(conv3,training=True)\n    conv3 = tf.keras.layers.ReLU()(conv3)\n    \n    conv4 = tf.keras.layers.Conv2D(in_channels * 2, kernel_size=1, dilation_rate=1, padding='valid',name= name_prefix + 'out_wav')(tf.concat([conv2, conv3], axis=3))\n    \n    return conv4, ll, lh, hl, hh   ","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.154472Z","iopub.execute_input":"2025-03-05T16:47:13.154961Z","iopub.status.idle":"2025-03-05T16:47:13.189625Z","shell.execute_reply.started":"2025-03-05T16:47:13.154914Z","shell.execute_reply":"2025-03-05T16:47:13.18856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dual-mode Attention Gate (DAG)","metadata":{}},{"cell_type":"code","source":"def conv_block(x, filter_size, size, dropout, batch_norm=False):\n    \n    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    conv = layers.ReLU()(conv)\n\n    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(conv)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    conv = layers.ReLU()(conv)\n    \n    if dropout > 0:\n        conv = layers.Dropout(dropout)(conv)\n\n    return conv\n\n\ndef repeat_elem(tensor, rep):\n     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),arguments={'repnum': rep})(tensor)\n\n\ndef res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n\n\n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    conv = layers.ReLU()(conv)\n    \n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n    if dropout > 0:\n        conv = layers.Dropout(dropout)(conv)\n\n    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n    if batch_norm is True:\n        shortcut = layers.BatchNormalization(axis=3)(shortcut,training=True)\n\n    res_path = layers.add([shortcut, conv])\n    res_path = layers.ReLU()(res_path)    #Activation after addition with shortcut (Original residual block)\n    return res_path\n\ndef gating_signal(input, out_size, batch_norm=False):\n    init = RandomNormal(stddev=0.02)\n    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n    if batch_norm:\n        x = layers.BatchNormalization(axis=3)(x,training=True)\n    x = layers.ReLU()(x)\n    return x\n\ndef attention_block(x, gating, inter_shape, name_prefix=''):\n    filters = x.shape[-1]\n    filtersg = gating.shape[-1]\n    shape_x = K.int_shape(x)\n    shape_g = K.int_shape(gating)\n    init = RandomNormal(stddev=0.02)\n    xa = x[:, :, :, :filters // 2]\n    xb = x[:, :, :, filters // 2: ]\n    gating_a = gating[:, :, :, :filtersg // 2]\n    gating_b = gating[:, :, :, filtersg // 2:]\n# Getting the x signal to the same shape as the gating signal\n    theta_xa = layers.Conv2D(inter_shape//2, (2, 2), strides=(2, 2), padding='same',name=name_prefix + 'theta_a')(xa)  # 16\n    shape_theta_xa = K.int_shape(theta_xa)\n    theta_xb = layers.Conv2D(inter_shape//2, (2, 2), strides=(2, 2), padding='same',name=name_prefix + 'theta_b')(xb)  # 16\n    shape_theta_xb = K.int_shape(theta_xb)\n# Getting the gating signal to the same number of filters as the inter_shape\n    phi_ga = layers.Conv2D(inter_shape//2, (1, 1), padding='same')(gating_a)\n    upsample_ga = layers.Conv2DTranspose(inter_shape//2, (3, 3),strides=(shape_theta_xa[1] // shape_g[1], shape_theta_xa[2] // shape_g[2]),padding='same',name=name_prefix + 'phi_ga')(phi_ga)  # 16\n    phi_gb = layers.Conv2D(inter_shape//2, (1, 1), padding='same')(gating_b)\n    upsample_gb = layers.Conv2DTranspose(inter_shape//2, (3, 3),strides=(shape_theta_xb[1] // shape_g[1], shape_theta_xb[2] // shape_g[2]),padding='same',name=name_prefix + 'phi_gb')(phi_gb)  # 16\n    \n    \n    ###################################################\n    concat_xg = layers.add([theta_xa,upsample_ga ],name=name_prefix + 'foreground_add')\n    act_xg = layers.ReLU()(concat_xg)\n    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n    sigmoid_xg = layers.Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi1 = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]),name=name_prefix + 'visual_fore')(sigmoid_xg)  # 32\n\n    upsample_psi = repeat_elem(upsample_psi1, xa.shape[3])\n    ya = layers.multiply([upsample_psi, xa],name=name_prefix + 'foreground_out')\n\n    \n    ##################################################\n    subtract_xg = layers.subtract([theta_xb,upsample_gb],name=name_prefix + 'background_add')\n    sub_act_xg = layers.ReLU()(subtract_xg)\n    sub_psi = layers.Conv2D(1, (1, 1), padding='same')(sub_act_xg)\n    sub_sigmoid_xg = layers.Activation('sigmoid',name=name_prefix + 'before_reverse')(sub_psi)\n    sub_sigmoid_xg = -1 * (sub_sigmoid_xg) + 1\n    sub_upsample_psi1 = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]),name=name_prefix +'visual_back' )(sub_sigmoid_xg)  # 32\n    sub_upsample_psi = repeat_elem(sub_upsample_psi1, xb.shape[3])\n    yb = layers.multiply([sub_upsample_psi, xb],name=name_prefix + 'background_out')\n    ##################################################\n    y = layers.Concatenate(axis=3)([ya, yb])\n\n    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n    result_bn = layers.BatchNormalization(axis=3)(result,training=True)\n    result_bn = layers.ReLU(name=name_prefix + 'attention_out')(result_bn)\n    return result_bn,upsample_psi1,sub_upsample_psi1","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.191167Z","iopub.execute_input":"2025-03-05T16:47:13.19153Z","iopub.status.idle":"2025-03-05T16:47:13.219836Z","shell.execute_reply.started":"2025-03-05T16:47:13.191473Z","shell.execute_reply":"2025-03-05T16:47:13.218676Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Other Blocks","metadata":{}},{"cell_type":"code","source":"def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1, 1),activation='relu', name=None):\n\n    init = RandomNormal(stddev=0.02)\n    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding,kernel_initializer=init )(x)\n    x = BatchNormalization(axis=3)(x, training=True)\n\n    if(activation == None):\n        return x\n\n    x = Activation(activation, name=name)(x)\n\n    return x\n\ndef trans_conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(2, 2), name=None,dropout=True):\n\n    x = Conv2DTranspose(filters, (num_row, num_col), strides=strides, padding=padding)(x)\n    x = BatchNormalization(axis=3)(x,training=True)\n    if dropout:\n        x = Dropout(0.5)(x, training=True)\n    \n    return x\n\n\ndef MultiResBlock(U, inp, alpha = 1.67):\n\n    W = alpha * U\n\n    shortcut = inp\n\n    shortcut = conv2d_bn(shortcut, int(W*0.167) + int(W*0.333) +\n                         int(W*0.5), 1, 1, activation=None, padding='same')\n\n    conv3x3 = conv2d_bn(inp, int(W*0.167), 3, 3,\n                        activation='relu', padding='same')\n\n    conv5x5 = conv2d_bn(conv3x3, int(W*0.333), 3, 3,\n                        activation='relu', padding='same')\n\n    conv7x7 = conv2d_bn(conv5x5, int(W*0.5), 3, 3,\n                        activation='relu', padding='same')\n\n    out = concatenate([conv3x3, conv5x5, conv7x7], axis=3)\n    out = BatchNormalization(axis=3)(out,training=True)\n\n    out = add([shortcut, out])\n    out = Activation('relu')(out)\n    out = BatchNormalization(axis=3)(out,training=True)\n\n    return out\n\ndef ResPath(filters, length, inp):\n\n    shortcut = inp\n    shortcut = conv2d_bn(shortcut, filters, 1, 1,activation=None, padding='same')\n\n    out = conv2d_bn(inp, filters, 3, 3, activation='relu', padding='same')\n\n    out = add([shortcut, out])\n    out = Activation('relu')(out)\n    out = BatchNormalization(axis=3)(out,training=True)\n\n    for i in range(length-1):\n\n        shortcut = out\n        shortcut = conv2d_bn(shortcut, filters, 1, 1,activation=None, padding='same')\n        out = conv2d_bn(out, filters, 3, 3, activation='relu', padding='same')\n        out = add([shortcut, out])\n        out = Activation('relu')(out)\n        out = BatchNormalization(axis=3)(out,training=True)\n\n    return out\n\ndef BasicConv2D(inputs,out_planes, kernel_size, stride=1, padding='same', dilation=1):\n    conv = tf.keras.layers.Conv2D(\n            filters=out_planes,\n            kernel_size=kernel_size,\n            strides=stride,\n            padding=padding,\n            dilation_rate=dilation,\n            use_bias=False\n        )(inputs)\n    bn = tf.keras.layers.BatchNormalization()(conv,training=True)\n    relu = tf.keras.layers.ReLU()(bn)\n    return relu\n    \n\ndef RFBModified(inputs, out_channel):\n    relu = tf.keras.layers.ReLU()(inputs)\n\n    # Define branch0\n    branch0 = BasicConv2D(relu, out_channel, kernel_size=1)\n\n    # Define branch1\n    conv1_1x1 = BasicConv2D(relu, out_channel, kernel_size=1)\n    conv1_1x3 = BasicConv2D(conv1_1x1, out_channel, kernel_size=(1, 3), padding='same')\n    conv1_3x1 = BasicConv2D(conv1_1x3, out_channel, kernel_size=(3, 1), padding='same')\n    conv1_3x3 = BasicConv2D(conv1_3x1, out_channel, kernel_size=3, padding='same', dilation=3)\n\n    # Define branch2\n    conv2_1x1 = BasicConv2D(relu, out_channel, kernel_size=1)\n    conv2_1x5 = BasicConv2D(conv2_1x1, out_channel, kernel_size=(1, 5), padding='same')\n    conv2_5x1 = BasicConv2D(conv2_1x5, out_channel, kernel_size=(5, 1), padding='same')\n    conv2_3x3 = BasicConv2D(conv2_5x1, out_channel, kernel_size=3, padding='same', dilation=5)\n\n    # Define branch3\n    conv3_1x1 = BasicConv2D(relu, out_channel, kernel_size=1)\n    conv3_1x7 = BasicConv2D(conv3_1x1, out_channel, kernel_size=(1, 7), padding='same')\n    conv3_7x1 = BasicConv2D(conv3_1x7, out_channel, kernel_size=(7, 1), padding='same')\n    conv3_3x3 = BasicConv2D(conv3_7x1, out_channel, kernel_size=3, padding='same', dilation=7)\n\n    # Concatenate branches\n    branches_concat = tf.keras.layers.Concatenate(axis=-1)([branch0, conv1_3x3, conv2_3x3, conv3_3x3])\n\n    # Final convolution and residual connection\n    conv_cat = BasicConv2D(branches_concat, out_channel, kernel_size=3, padding='same')\n    conv_res = BasicConv2D(relu, out_channel, kernel_size=1)\n\n    # Output\n    output = tf.keras.layers.ReLU()(conv_cat + conv_res)\n\n    return output\ndef aggregation(x1, x2, x3):\n    channel=32\n    upsample = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')\n\n    x1_1 = x1\n    x1_1_1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_1)\n    x1_1_2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_1_1)\n    x2_1_1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x2)\n    x2_1 = tf.math.multiply(BasicConv2D(x1_1_1 ,channel, 3, padding='same'), x2)\n    x3_1 = BasicConv2D(x1_1_2,channel, 3, padding='same') \n    x3_1 = tf.math.multiply(x3_1 , BasicConv2D(x2_1_1,channel, 3, padding='same'))\n    x3_1 = tf.math.multiply(x3_1, x3)\n\n    x2_2 = tf.concat([x2_1, BasicConv2D(upsample(x1_1),2*channel, 3, padding='same')], axis=-1)\n    x2_2 = BasicConv2D(x2_2,2 * channel, 3, padding='same')\n\n    x3_2 = tf.concat([x3_1, BasicConv2D(upsample(x2_2),2 * channel, 3, padding='same')], axis=-1)\n    x3_2 = BasicConv2D(x3_2,3 * channel, 3, padding='same')\n\n    x = BasicConv2D(x3_2,3 * channel, 3, padding='same')\n    x = tf.keras.layers.Conv2D(1, 1)(x)\n    \n    return x\n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.221577Z","iopub.execute_input":"2025-03-05T16:47:13.221888Z","iopub.status.idle":"2025-03-05T16:47:13.251662Z","shell.execute_reply.started":"2025-03-05T16:47:13.22186Z","shell.execute_reply":"2025-03-05T16:47:13.250534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GAN Architecture","metadata":{}},{"cell_type":"code","source":"def define_discriminator(image_shape):\n    \n    # weight initialization\n    init = RandomNormal(stddev=0.02) \n    # source image input\n    in_src_image = Input(shape=image_shape) \n    # target image input\n    in_target_image = Input(shape=(image_shape[0],image_shape[1],1)) \n    \n    # concatenate images, channel-wise\n    merged = Concatenate()([in_src_image, in_target_image])\n    \n    # C64: 4x4 kernel Stride 2x2\n    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C128: 4x4 kernel Stride 2x2\n    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = BatchNormalization()(d)\n    \n    # C256: 4x4 kernel Stride 2x2\n    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = BatchNormalization()(d)\n    \n    # C512: 4x4 kernel Stride 2x2 \n    # Not in the original paper. Comment this block if you want.\n    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = BatchNormalization()(d)\n    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n    patch_out = Activation('sigmoid')(d)\n    # define model\n    model = Model([in_src_image, in_target_image], patch_out)\n    # compile model\n    opt = Adam(lr=0.000009, beta_1=0.5)\n    model.compile(loss='mae', optimizer=opt, loss_weights=[0.5])\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.253249Z","iopub.execute_input":"2025-03-05T16:47:13.253566Z","iopub.status.idle":"2025-03-05T16:47:13.270381Z","shell.execute_reply.started":"2025-03-05T16:47:13.25354Z","shell.execute_reply":"2025-03-05T16:47:13.269543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def define_generator(height, width, n_channels,name_prefix='f_'):\n    inputs = Input((height, width, n_channels))\n    encoder = VGG16(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n    encoder1 = DenseNet201(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n                           \n    mresblock1 = MultiResBlock(32, inputs)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(mresblock1)\n    mresblock1 = ResPath(32, 4, mresblock1)\n\n    \n    ######################################\n    wav_pool256 = DWT(name=\"haar\",concat=0)(inputs)\n    s2 = encoder.get_layer(\"block2_conv2\").output \n    e2 = encoder1.get_layer(\"conv1/relu\").output\n    mresblock2 = Concatenate()([pool1,s2,e2])\n    mresblock2 = MultiResBlock(64, mresblock2)\n    mresblock2 = Concatenate()([wav_pool256, mresblock2])\n    pool2 = MaxPooling2D(pool_size=(2, 2))(mresblock2)\n    mresblock2 = ResPath(64, 3, mresblock2)\n\n    ###########################################\n    mres_wav_pool256= DWT(concat=2)(mresblock1)\n    wav_cnn128 = Concatenate(name= name_prefix + 'wav_2_in')([wav_pool256, mresblock2,mres_wav_pool256])\n    wav_cnn128,_,_,_,_ = wavelet_conv_block(wav_cnn128, 16,name_prefix= name_prefix + 'wav_2')\n    \n    wav_out1_ll = DWT(concat=2)(inputs)\n    wav_pool128 = DWT(concat=0)(wav_out1_ll)\n    \n    s3 = encoder.get_layer(\"block3_conv2\").output\n    e3 = encoder1.get_layer(\"pool2_conv\").output\n    mresblock3 = Concatenate()([pool2,s3,e3])\n    mresblock3 = MultiResBlock(128, mresblock3)\n    mresblock3 = Concatenate()([wav_cnn128, mresblock3])\n    pool3 = MaxPooling2D(pool_size=(2, 2))(mresblock3)\n    mresblock3 = ResPath(128, 2, mresblock3)\n\n    #############################################\n    \n    \n    mres_wav_pool128 = DWT(concat=2)(mresblock2)\n    wav_cnn64 = Concatenate()([wav_pool128, mresblock3,mres_wav_pool128])\n    wav_cnn64,_,_,_,_ = wavelet_conv_block(wav_cnn64, 32,name_prefix= name_prefix + 'wav_3')    \n    \n    wav_out2_ll = DWT(concat=2)(wav_out1_ll)\n    wav_pool64 = DWT(concat=0)(wav_out2_ll)\n    \n    s4 = encoder.get_layer(\"block4_conv2\").output\n    e4 = encoder1.get_layer(\"pool3_conv\").output\n    mresblock4= Concatenate()([pool3,s4,e4])\n    mresblock4 = MultiResBlock(256, mresblock4)\n    mresblock4= Concatenate()([wav_cnn64, mresblock4])\n    pool4 = MaxPooling2D(pool_size=(2, 2))(mresblock4)\n    mresblock4 = ResPath(256, 1, mresblock4)\n    \n    \n    ###############################################\n    \n    mres_wav_pool64 = DWT(concat=2)(mresblock3)\n    wav_cnn32 = Concatenate()([wav_pool64, mresblock4,mres_wav_pool64])\n    wav_cnn32,_,_,_,_ = wavelet_conv_block(wav_cnn32, 64,name_prefix= name_prefix + 'wav_4')\n    \n    s5 = encoder.get_layer(\"block5_conv2\").output\n    e5 = encoder1.get_layer(\"pool4_conv\").output\n    mresblock5= Concatenate()([pool4,s5,e5])\n    mresblock5 = MultiResBlock(512, mresblock5)\n    mresblock5= Concatenate()([wav_cnn32, mresblock5])\n    pool5 = MaxPooling2D(pool_size=(2, 2))(mresblock5)\n    mresblock5 = ResPath(512, 1, mresblock5)\n    ##############################\n\n    mresblock6 = MultiResBlock(1024, pool5)\n\n    ###########################################\n    \n    gating_8 = gating_signal(mresblock6, 512, batch_norm=True)\n    att_8,_,_ = attention_block(mresblock5, gating_8, 512,name_prefix = name_prefix + 'at_1')\n    up5 = concatenate([trans_conv2d_bn(mresblock6, filters=512, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=True), att_8], axis=3)\n    mresblock7 = MultiResBlock(512, up5) \n    \n    \n    \n    \n    gating_16 = gating_signal(mresblock7, 256, batch_norm=True)\n    att_16,_,_ = attention_block(mresblock4, gating_16, 256,name_prefix = name_prefix + 'at_2')\n    up6 = concatenate([trans_conv2d_bn(mresblock7, filters=256, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=True), att_16], axis=3)\n    mresblock8 = MultiResBlock(256, up6)\n    \n    \n\n    gating_32 = gating_signal(mresblock8, 128, batch_norm=True)\n    att_32,at3_fore,at3_back = attention_block(mresblock3, gating_32, 128,name_prefix = name_prefix + 'at_3')\n    up7 = concatenate([trans_conv2d_bn(mresblock8, filters=128, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=True), att_32], axis=3)\n    mresblock9 = MultiResBlock(128, up7)\n\n\n    \n    \n    \n    gating_64 = gating_signal(mresblock9, 64, batch_norm=True)\n    att_64,at4_fore,at4_back = attention_block(mresblock2, gating_64, 64,name_prefix = name_prefix + 'at_4')\n    up8 = concatenate([trans_conv2d_bn(mresblock9, filters=64, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=False), att_64], axis=3)\n    mresblock10 = MultiResBlock(64, up8)\n\n    \n    \n\n    \n    \n    gating_128 = gating_signal(mresblock10, 32, batch_norm=True)\n    att_128,at5_fore,at5_back = attention_block(mresblock1, gating_128, 32,name_prefix = name_prefix + 'at_5')\n    up9 = concatenate([trans_conv2d_bn(mresblock10, filters=32, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=False), att_128], axis=3)\n    mresblock11 = MultiResBlock(32, up9)\n    \n    \n    init = RandomNormal(stddev=0.02)\n    conv10 = conv2d_bn(mresblock11, 1, 1, 1, activation='sigmoid')\n                           \n    \n    x3_rfb=RFBModified(mresblock3,32)\n    x4_rfb=RFBModified(mresblock4,32)\n    x5_rfb=RFBModified(mresblock5,32)\n\n    ra5_feat = aggregation(x5_rfb,x4_rfb,x3_rfb)\n\n\n    crop_5 = tf.image.resize(ra5_feat, [height//16,width//16])\n    x = -1 * (tf.math.sigmoid(crop_5)) + 1\n    x = tf.keras.layers.Multiply()([x, mresblock5])\n    \n    x = BasicConv2D(x, 256, 1)\n    x = BasicConv2D(x, 256, 5, padding='same')\n    x = BasicConv2D(x, 256, 5, padding='same')\n    x = BasicConv2D(x, 256, 5, padding='same')\n    ra4_feat = BasicConv2D(x, 1, 1) ## deep supervise this for better result\n    x = tf.keras.layers.Add()([ra4_feat, crop_5])\n    crop_4 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n    x = -1 * (tf.math.sigmoid(crop_4)) + 1\n    x = tf.keras.layers.Multiply()([x, mresblock4])\n    x = BasicConv2D(x, 64, 1)\n    x = BasicConv2D(x, 64, 3, padding='same')\n    x = BasicConv2D(x, 64, 3, padding='same')\n    ra3_feat = BasicConv2D(x, 1, 3, padding='same')  ## deep supervise this for better result\n    x = tf.keras.layers.Add()([ra3_feat, crop_4])\n\n    crop_3 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n    x = -1 * (tf.math.sigmoid(crop_3)) + 1\n    x = tf.keras.layers.Multiply()([x, mresblock3])\n    x = BasicConv2D(x, 64, 1)\n    x = BasicConv2D(x, 64, 3, padding='same')\n    x = BasicConv2D(x, 64, 3, padding='same')\n    ra2_feat = BasicConv2D(x, 1, 3, padding='same')  ## deep supervise this for better result\n    x = tf.keras.layers.Add()([ra2_feat, crop_3])\n    lateral_map_2 = tf.keras.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x)\n    lateral_map_2 = tf.keras.activations.sigmoid(lateral_map_2)\n\n    out_image=conv10*lateral_map_2\n\n    model = Model(inputs, [out_image,at5_fore,at5_back,at4_fore,at4_back,at3_fore,at3_back])\n    \n\n    return model\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.273045Z","iopub.execute_input":"2025-03-05T16:47:13.273336Z","iopub.status.idle":"2025-03-05T16:47:13.305674Z","shell.execute_reply.started":"2025-03-05T16:47:13.273311Z","shell.execute_reply":"2025-03-05T16:47:13.304627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def define_gan(g_model, d_model, image_shape):\n    # make weights in the discriminator not trainable\n    for layer in d_model.layers:\n        if not isinstance(layer, BatchNormalization):\n            layer.trainable = False       #Descriminator layers set to untrainable in the combined GAN but \n                                                #standalone descriminator will be trainable.\n            \n    # define the source image\n    in_src = Input(shape=image_shape)\n    # suppy the image as input to the generator \n    gen_out = g_model(in_src)\n    # supply the input image and generated image as inputs to the discriminator\n    dis_out = d_model([in_src, gen_out[0]])\n    # src image as input, generated image and disc. output as outputs\n    model = Model(in_src, [dis_out, gen_out[0], gen_out[1], gen_out[2], gen_out[3], gen_out[4], gen_out[5], gen_out[6]])\n    # compile model\n    opt = Adam(lr=0.00008, beta_1=0.5)\n    \n    model.compile(loss=['binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy'],optimizer=opt, loss_weights=[1,25,5,5,5,5,3,3])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.306831Z","iopub.execute_input":"2025-03-05T16:47:13.307135Z","iopub.status.idle":"2025-03-05T16:47:13.321776Z","shell.execute_reply.started":"2025-03-05T16:47:13.307108Z","shell.execute_reply":"2025-03-05T16:47:13.320795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Augmentation and data loader","metadata":{}},{"cell_type":"code","source":"\naug1 = A.HorizontalFlip(p=1)\naug2 = A.VerticalFlip(p=1)\naug3 = A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1)\naug4 = A.Blur(blur_limit=11, always_apply=True, p=1)\naug5 = A.Cutout(num_holes=8, max_h_size=96, max_w_size=96, fill_value=0, always_apply=True, p=1)\naug6 = A.Rotate(limit=90, interpolation=1, border_mode=2, value=None, mask_value=None, rotate_method='largest_box', crop_border=False, always_apply=True, p=1)\naug7 = A.Downscale(scale_min=0.25, scale_max=0.25, interpolation=None, always_apply=True, p=1)\naug8 = A.RandomBrightnessContrast (brightness_limit=0.4, contrast_limit=0.3, brightness_by_max=True, always_apply=True, p=1)\naug9 = A.HueSaturationValue(hue_shift_limit=0.3, sat_shift_limit=0.4, val_shift_limit=0.3, always_apply=True, p=1)\naug10 = A.Affine(scale=(0.2,0.3), translate_percent=0.2, rotate=(-30,30), shear=(-45,45), always_apply=True, p=1)\n\n# I used one augmentation type at a time, you can do multiple\ndef augment(image,mask,n):\n    if n==0:\n        augmented = aug10(image=image, mask=mask)\n    elif n==1:\n        augmented = aug1(image=image, mask=mask)\n    elif n==2: \n        augmented = aug2(image=image, mask=mask)\n    elif n==3: \n        augmented = aug3(image=image, mask=mask)\n    elif n==6:\n        augmented = aug6(image=image, mask=mask)\n    elif n==4: \n        augmented = aug4(image=image, mask=mask)\n    elif n==5: \n        augmented = aug5(image=image, mask=mask)\n    elif n==7:\n        return image,mask\n    elif n==8:\n        augmented = aug7(image=image, mask=mask)\n    elif n==9:\n        augmented = aug8(image=image, mask=mask)\n    elif n==10: \n        augmented = aug6(image=image, mask=mask)\n    else: \n        augmented = aug9(image=image, mask=mask)\n    \n        \n    image_aug= augmented['image']\n    mask_aug = augmented['mask']\n    return image_aug,mask_aug\n\ndef generate_real_samples(data_generator,mask_generator, n_samples,patch_shape, i):\n    ix = randint(0, len(data_generator), n_samples)\n    n = i % 12\n\n    # Initialize lists to store augmented images and masks\n    X1 = []\n    X2 = []\n    X3 = []\n    X4 = []\n    X5 = []\n    X6 = []\n    X7 = []\n    for i in ix:\n        augmented_image, augmented_mask = augment(\n            np.reshape(data_generator[i], [IMAGE_HEIGHT,IMAGE_WIDTH, 3]),  # Input image\n            np.reshape(mask_generator[i], [IMAGE_HEIGHT,IMAGE_WIDTH, 1]),  # Input mask\n            n  # Augmentation type\n        )\n        X1.append(augmented_image)\n        X2.append(augmented_mask)\n        X3.append(1-augmented_mask)\n        X4.append(tf.image.resize(augmented_mask,(IMAGE_HEIGHT//2,IMAGE_WIDTH//2)))\n        X5.append(tf.image.resize(1-augmented_mask,(IMAGE_HEIGHT//2,IMAGE_WIDTH//2)))\n        X6.append(tf.image.resize(augmented_mask,(IMAGE_HEIGHT//4,IMAGE_WIDTH//4)))\n        X7.append(tf.image.resize(1-augmented_mask,(IMAGE_HEIGHT//4,IMAGE_WIDTH//4)))\n    y = ones((n_samples, patch_shape, patch_shape, 1))\n    return [np.array(X1), np.array(X2), np.array(X3), np.array(X4), np.array(X5), np.array(X6), np.array(X7)],y\n\n\n# generate a batch of images, returns images and targets\ndef generate_fake_samples(g_model, samples, patch_shape):\n    # generate fake instance\n    X = g_model.predict(samples)\n    # create 'fake' class labels (0)\n    y = zeros((BATCH_SIZE, patch_shape, patch_shape, 1))\n    return X[0], y\n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.323351Z","iopub.execute_input":"2025-03-05T16:47:13.32367Z","iopub.status.idle":"2025-03-05T16:47:13.346232Z","shell.execute_reply.started":"2025-03-05T16:47:13.323644Z","shell.execute_reply":"2025-03-05T16:47:13.344952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain1 = glob.glob(TRAIN_IMAGE_PATH+ '/*.png')  # Adjust the file extension if needed\ntrain2 = glob.glob(TRAIN_MASK_PATH + '/*.png')\nval1 = glob.glob(VAL_IMAGE_PATH + '/*.png')\nval2 =glob.glob(VAL_MASK_PATH + '/*.png')\ntest1=glob.glob(TEST_IMAGE_PATH + '/*.png')\ntest2=glob.glob(TEST_MASK_PATH + '/*.png')\n\ntrain1 = natsort.natsorted(train1)\ntrain2 = natsort.natsorted(train2)\nval1 = natsort.natsorted(val1)\nval2 = natsort.natsorted(val2)\ntest1 = natsort.natsorted(test1)\ntest2 = natsort.natsorted(test2)\n# Split the data into train and test sets\ndf_image_train = pd.DataFrame(train1, columns=['path'])\ndf_mask_train = pd.DataFrame(train2, columns=['path'])\ndf_image_val = pd.DataFrame(val1, columns=['path'])\ndf_mask_val = pd.DataFrame(val2, columns=['path'])\ndf_image_test = pd.DataFrame(test1, columns=['path'])\ndf_mask_test = pd.DataFrame(test2, columns=['path'])\n\nimggen = ImageDataGenerator(rescale=1./255)\n\ntrain_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_image_train,\n    x_col='path',\n    y_col=None,\n    directory=TRAIN_IMAGE_PATH,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n    color_mode='rgb',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\n\ntest_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_image_test,\n    x_col='path',\n    y_col=None,\n    directory=TEST_IMAGE_PATH,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n    color_mode='rgb',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\nvalid_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_image_val,\n    x_col='path',\n    y_col=None,\n    directory=VAL_IMAGE_PATH,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n    color_mode='rgb',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\n# Create ImageDataGenerator for masks\nmask_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_mask_train,\n    x_col='path',\n    y_col=None,\n    directory=TRAIN_MASK_PATH,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\n\nmask_data_generator_test = imggen.flow_from_dataframe(\n    dataframe=df_mask_test,\n    x_col='path',\n    y_col=None,\n    directory=TEST_MASK_PATH,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\nmask_data_generator_valid = imggen.flow_from_dataframe(\n    dataframe=df_mask_val,\n    x_col='path',\n    y_col=None,\n    directory=VAL_MASK_PATH,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:13.34796Z","iopub.execute_input":"2025-03-05T16:47:13.348291Z","iopub.status.idle":"2025-03-05T16:47:14.7194Z","shell.execute_reply.started":"2025-03-05T16:47:13.348255Z","shell.execute_reply":"2025-03-05T16:47:14.718313Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def summarize_performance(step, g_model1,g_model2):\n\n    filename2 = 'lesion_model_%06d.h5' % (step+1)\n    g_model1.save(filename2)\n    filename4 = 'background_model_%06d.h5' % (step+1)\n    g_model2.save(filename4)\n    print('>Saved: %s and %s' % (filename2,filename4))\n\ndef remove_prev_performance(score_epoch):\n    filename3 = 'lesion_model_%06d.h5' % (score_epoch)\n    filename5 = 'background_model_%06d.h5' % (score_epoch)\n    os.remove(\"/kaggle/working/\"+filename3)\n    os.remove(\"/kaggle/working/\"+filename5)\n    print('>Removed: %s and %s and updated' % (filename3,filename5))\n\ndef summarize_performancet(step, g_model1,g_model2,d_model1,d_model2):\n\n    filename2 = 'lesion_model_%06d.h5' % (step+1)\n    g_model1.save(filename2)\n    filename4 = 'background_model_%06d.h5' % (step+1)\n    g_model2.save(filename4)\n    filename1 = 'lesion_dis_%06d.h5' % (step+1)\n    d_model1.save(filename1)\n    filename3 = 'background_dis_%06d.h5' % (step+1)\n    d_model2.save(filename3)\n    print('>Saved: %s and %s , %s and %s' % (filename2,filename4,filename1,filename3))\n\ndef remove_prev_performancet(score_epoch):\n    filename3 = 'lesion_model_%06d.h5' % (score_epoch)\n    filename5 = 'background_model_%06d.h5' % (score_epoch)\n    filename7 = 'lesion_dis_%06d.h5' % (score_epoch)\n    filename9 = 'background_dis_%06d.h5' % (score_epoch)\n    os.remove(\"/kaggle/working/\"+filename3)\n    os.remove(\"/kaggle/working/\"+filename5)\n    os.remove(\"/kaggle/working/\"+filename7)\n    os.remove(\"/kaggle/working/\"+filename9)\n    print('>Removed: %s and %s, %s and %s and updated' % (filename3,filename5,filename7,filename9))    \n    \n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:14.720824Z","iopub.execute_input":"2025-03-05T16:47:14.72112Z","iopub.status.idle":"2025-03-05T16:47:14.732576Z","shell.execute_reply.started":"2025-03-05T16:47:14.721095Z","shell.execute_reply":"2025-03-05T16:47:14.731366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Additional Losses","metadata":{}},{"cell_type":"code","source":"from tensorflow.image import ssim\n\ndef log_ssim_mse_loss(y_true, y_pred, alpha=0.4):\n    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n    loss = -tf.math.log(ssim) * alpha + mse * (1 - alpha)\n    return loss*5\n\ndef boundary_loss(y_true, y_pred):\n    # Compute the gradient of the predicted and ground truth masks\n    dy_true, dx_true = tf.image.image_gradients(y_true)\n    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n\n    # Compute the boundary term of the loss function\n    term_1 = tf.abs(tf.reduce_mean(tf.abs(dy_true) - tf.abs(dy_pred)))\n    term_2 = tf.abs(tf.reduce_mean(tf.abs(dx_true) - tf.abs(dx_pred)))\n\n    # Return the sum of the two terms as the boundary loss\n    return (term_1 + term_2)*5\n\n\ndef mae_loss(y_true, y_pred):\n    return mean_absolute_error(tf.reshape(y_true, [-1]), tf.reshape(y_pred, [-1]))","metadata":{"execution":{"iopub.status.busy":"2025-03-05T16:47:14.734221Z","iopub.execute_input":"2025-03-05T16:47:14.734651Z","iopub.status.idle":"2025-03-05T16:47:14.749882Z","shell.execute_reply.started":"2025-03-05T16:47:14.73462Z","shell.execute_reply":"2025-03-05T16:47:14.748769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"\ndef train(d_model, g_model, gan_model,d_model2,g_model2,gan_model2, train_data_generator,mask_data_generator, n_epochs=100, n_batch=1):\n    n_patch = d_model.output_shape[1]\n    bat_per_epo = int(len(train_data_generator)/ n_batch)\n    # calculate the number of training iterations\n    n_steps = bat_per_epo * n_epochs\n    print('starting')\n    dloss11=[]\n    dloss21=[]\n    gloss1=[]\n    score1=[]\n    Bdloss11=[]\n    Bdloss21=[]\n    rloss11=[]\n    Bgloss1=[]\n    Bscore1=[]\n    merge_score=[]\n    epochss=[]\n    epochss = list(range(0,n_epochs))\n    score_max=0\n    score_epoch=0\n    # manually enumerate epochs\n    for j in range(n_epochs):\n        dloss1=[]\n        dloss2=[]\n        gloss=[]\n        Bdloss1=[]\n        Bdloss2=[]\n        Bgloss=[]\n        rloss=[]\n        iou_score=[]\n        iou_scoreB=[]\n        iou_score_merge=[]\n        maxv1=0\n        count1=0\n        maxv2=0\n        count2=0\n        for i in range(bat_per_epo):\n        ##for lesion\n        # select a batch of real samples\n            [X_realA, X_realB, X_realC,X_realB256,X_realC256,X_realB128,X_realC128], y_real = generate_real_samples(train_data_generator,mask_data_generator, n_batch, n_patch,i)\n        # generate a batch of fake samples\n            X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n            X_fakeC, y_fake = generate_fake_samples(g_model2, X_realA, n_patch)\n        # update discriminator for real samples\n            d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n        # update discriminator for generated samples\n            d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n            \n            Bd_loss1 = d_model2.train_on_batch([X_realA, X_realC], y_real)\n        # update discriminator for generated samples\n            Bd_loss2 = d_model2.train_on_batch([X_realA, X_fakeC], y_fake)\n        # update the generator\n        \n            X_realA_s = (X_realA)\n            X_fakeB1 = (X_fakeB)\n            X_fakeB1 = np.around(X_fakeB1)\n            BX_fakeB1 = (X_fakeC)\n            BX_fakeB1 = np.around(BX_fakeB1)\n            X_fakeBnew = tf.cast(np.logical_or(X_fakeB1,BX_fakeB1), tf.float32) \n            X_realA_recons = tf.cast(np.multiply(X_realA_s,X_fakeBnew),tf.float32)\n            BX_fakeB2 = (1-X_fakeC)\n            BX_fakeB2 = (BX_fakeB2)\n            BX_fakeB2 = np.around(BX_fakeB2)\n            X_fakeBnew2 = tf.cast(np.logical_and(X_fakeB1,BX_fakeB2), tf.float32) \n            reconstruction_loss = log_ssim_mse_loss(X_realA_s, X_realA_recons) + (tf.keras.backend.mean(tf.keras.losses.MAE( tf.cast(X_realB, tf.float32), X_fakeBnew2))*2) + boundary_loss(tf.cast(X_realB, tf.float32), X_fakeBnew2)\n            gan_model.add_loss(lambda:reconstruction_loss)\n            gan_model2.add_loss(lambda:reconstruction_loss)  \n            \n            g_loss, _, _, _, _, _, _ , _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB,X_realB,X_realB,X_realB256,X_realC256,X_realB128,X_realB128])\n            Bg_loss, _, _, _, _, _, _, _, _ = gan_model2.train_on_batch(X_realA, [y_real, X_realC,X_realC,X_realC,X_realB256,X_realC256,X_realC128,X_realC128])\n\n\n            dloss1.append(d_loss1)\n            dloss2.append(d_loss2)\n            gloss.append(g_loss)\n            rloss.append(reconstruction_loss)\n            Bdloss1.append(Bd_loss1)\n            Bdloss2.append(Bd_loss2)\n            Bgloss.append(Bg_loss)\n\n        test_iou_score1and=[]\n        test_iou_score1or=[]\n        testf=[]\n        testb=[]\n        l=0\n        for l in range(len(test_data_generator)):\n            output=tf.reshape(test_data_generator[l], [1, IMAGE_HEIGHT,IMAGE_WIDTH, 3])\n            gen_lesion_image1 = g_model.predict(output)\n            gen_back_image1 = g_model2.predict(output)\n            gen_lesion_image1 = (gen_lesion_image1[0])\n            gen_lesion_image1 = np.around(gen_lesion_image1)\n            gen_back_image1 = (1-gen_back_image1[0])\n            gen_back_image1 = np.around(gen_back_image1)\n            \n            tar_image1 = np.around(mask_data_generator_test[l])\n            gen_image1 = numpy.logical_and(gen_lesion_image1,gen_back_image1 )\n            gen_image1or = numpy.logical_or(gen_lesion_image1,gen_back_image1 )\n            intersection1 = numpy.logical_and(gen_image1, tar_image1)\n            union1 = numpy.logical_or(gen_image1, tar_image1)\n            test_iou_score1and.append(numpy.sum(intersection1) / numpy.sum(union1))\n            intersection1or = numpy.logical_and(gen_image1or, tar_image1)\n            union1or = numpy.logical_or(gen_image1or, tar_image1)\n            test_iou_score1or.append(numpy.sum(intersection1or) / numpy.sum(union1or))\n        test_iou_score2and=numpy.mean(test_iou_score1and)\n        test_iou_score2or=numpy.mean(test_iou_score1or)\n        \n        valid_iou_score1and=[]\n        valid_iou_score1or=[]\n        l=0\n        for l in range(len(valid_data_generator)):\n            output=tf.reshape(valid_data_generator[l], [1, IMAGE_HEIGHT,IMAGE_WIDTH, 3])\n            gen_lesion_image1 = g_model.predict(output)\n            gen_back_image1 = g_model2.predict(output)\n            gen_lesion_image1 = (gen_lesion_image1[0])\n            gen_lesion_image1 = np.around(gen_lesion_image1)\n            gen_back_image1 = (1-gen_back_image1[0])\n            gen_back_image1 = np.around(gen_back_image1)\n            \n            tar_image1 = np.around(mask_data_generator_valid[l])\n            gen_image1 = numpy.logical_and(gen_lesion_image1,gen_back_image1 )\n            gen_image1or = numpy.logical_or(gen_lesion_image1,gen_back_image1)\n            intersection1 = numpy.logical_and(gen_image1, tar_image1)\n            union1 = numpy.logical_or(gen_image1, tar_image1)\n            valid_iou_score1and.append(numpy.sum(intersection1) / numpy.sum(union1))\n            intersection1or = numpy.logical_and(gen_image1or, tar_image1)\n            union1or = numpy.logical_or(gen_image1or, tar_image1)\n            valid_iou_score1or.append(numpy.sum(intersection1or) / numpy.sum(union1or))\n        valid_iou_score2and=numpy.mean(valid_iou_score1and)\n        valid_iou_score2or=numpy.mean(valid_iou_score1or)\n        \n        lr_scheduler_f.on_epoch_end(j+1, {'val_iou': max(valid_iou_score2and,valid_iou_score2or)})\n        lr_scheduler_b.on_epoch_end(j+1, {'val_iou': max(valid_iou_score2and,valid_iou_score2or)})\n\n        dloss1m=numpy.mean(dloss1)\n        dloss2m=numpy.mean(dloss2)\n        glossm=numpy.mean(gloss)\n        Bdloss1m=numpy.mean(Bdloss1)\n        Bdloss2m=numpy.mean(Bdloss2)\n        rlossm=numpy.mean(rloss)\n        Bglossm=numpy.mean(Bgloss)\n        print('Epoch %d> d1[%.3f] d2[%.3f]  g[%.3f] Bd1[%.3f] Bd2[%.3f]  Bg[%.3f]  r[%.3f] valid and:[%.3f] valid or:[%.3f]  test_iou_and:[%.5f] test_iou_OR:[%.5f]' % (j+1, dloss1m, dloss2m, glossm,Bdloss1m, Bdloss2m,  Bglossm,rlossm, valid_iou_score2and,valid_iou_score2or,test_iou_score2and,test_iou_score2or))\n        dloss11.append(dloss1m)\n        dloss21.append(dloss2m)\n        gloss1.append(glossm)\n        Bdloss11.append(Bdloss1m)\n        Bdloss21.append(Bdloss2m)\n        rloss11.append(rlossm)\n        Bgloss1.append(Bglossm)\n        # summarize performance\n        # best validation score based decision // recommended\n        if max(valid_iou_score2and,valid_iou_score2or)>=score_max:\n            summarize_performance((j), g_model,g_model2)\n            if j!=0:\n                remove_prev_performance(score_epoch)\n            score_max=max(valid_iou_score2and,valid_iou_score2or)\n            score_epoch=j+1\n        # best test score based decision // cherry picking\n        if max(test_iou_score2and,test_iou_score2or)>=score_max:\n            summarize_performance((j), g_model,g_model2)\n            if j!=0:\n                remove_prev_performance(score_epoch)\n            score_max=max(test_iou_score2and,test_iou_score2or)\n            score_epoch=j+1\n    \n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T17:01:59.302788Z","iopub.execute_input":"2025-03-05T17:01:59.30318Z","iopub.status.idle":"2025-03-05T17:01:59.3335Z","shell.execute_reply.started":"2025-03-05T17:01:59.303148Z","shell.execute_reply":"2025-03-05T17:01:59.332367Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calling everything to run","metadata":{}},{"cell_type":"code","source":"image_shape = (IMAGE_HEIGHT,IMAGE_WIDTH,3)\n# define the models\nd_model_fore = define_discriminator(image_shape)\ng_model_fore = define_generator(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, n_channels=3,name_prefix='f_')\nd_model_back = define_discriminator(image_shape)\ng_model_back = define_generator(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, n_channels=3,name_prefix='b_')\ngan_model_fore = define_gan(g_model_fore, d_model_fore, image_shape)\ngan_model_back = define_gan(g_model_back, d_model_back, image_shape)\n\nfrom tensorflow.keras.callbacks import Callback \nclass LearningRateSchedulerWithPatienceIoU(Callback):\n    def __init__(self, reduce_lr_factor, patience, min_lr, monitor='val_iou',model= None):\n        super(LearningRateSchedulerWithPatienceIoU, self).__init__()\n        self.reduce_lr_factor = reduce_lr_factor\n        self.patience = patience\n        self.min_lr = min_lr\n        self.monitor = monitor\n        self.wait = 0\n        self.best_iou = -float('inf')\n        self.model = model\n    def on_epoch_end(self, epoch, logs=None):\n        current_iou = logs.get(self.monitor, -float('inf'))\n\n        if current_iou > self.best_iou:\n            self.best_iou = current_iou\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                new_lr = self.model.optimizer.lr.numpy() * self.reduce_lr_factor\n                new_lr = max(new_lr, self.min_lr)\n                self.model.optimizer.lr.assign(new_lr)\n                print(f\"\\nReduced learning rate to {new_lr} after {self.patience} epochs without improvement in IoU.\\n\")\n                self.wait = 0\n\n\ninitial_learning_rate = 0.00008  # Initial learning rate\nreduce_lr_factor = 0.5  # Factor by which to reduce the learning rate\npatience = 30  # Number of epochs without improvement in IoU before reducing learning rate\nmin_learning_rate = 1e-8  # Minimum learning rate\n\n# Create the custom learning rate scheduler\nlr_scheduler_f = LearningRateSchedulerWithPatienceIoU(reduce_lr_factor, patience, min_learning_rate,model=gan_model_fore)\nlr_scheduler_b = LearningRateSchedulerWithPatienceIoU(reduce_lr_factor, patience, min_learning_rate,model=gan_model_back)\n\n\nfrom datetime import datetime \nstart1 = datetime.now() \n\ntrain(d_model_fore, g_model_fore, gan_model_fore,d_model_back,g_model_back,gan_model_back,train_data_generator,mask_data_generator, n_epochs=EPOCHS, n_batch=BATCH_SIZE) \n\n\nstop1 = datetime.now()\n#Execution time of the model \nexecution_time = stop1-start1\nprint(\"Execution time is: \", execution_time)","metadata":{"execution":{"iopub.status.busy":"2025-03-05T17:02:02.375346Z","iopub.execute_input":"2025-03-05T17:02:02.375808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}